# SONAR-LLM Experiments

Professional implementation of SONAR-LLM training on RULER benchmark tasks.

## ğŸ“ Project Structure

```
sonar_llm_experiments/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ niah_config.json       # NIAH task configuration
â”‚   â””â”€â”€ qa_config.json         # QA task configuration
â”œâ”€â”€ scripts/                    # Python scripts
â”‚   â”œâ”€â”€ text_cleaning.py       # Text cleaning utilities (FlagEmbedding-style)
â”‚   â”œâ”€â”€ generate_niah_data.py  # NIAH dataset generation
â”‚   â”œâ”€â”€ generate_qa_data.py    # QA dataset generation
â”‚   â”œâ”€â”€ train_niah.py          # NIAH training script
â”‚   â””â”€â”€ train_qa.py            # QA training script
â”œâ”€â”€ data/                       # Generated datasets
â”‚   â”œâ”€â”€ niah/                  # NIAH datasets
â”‚   â””â”€â”€ qa/                    # QA datasets
â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ niah_model/            # NIAH checkpoints
â”‚   â””â”€â”€ qa_model/              # QA checkpoints
â”œâ”€â”€ results/                    # Evaluation results
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### 1. Generate NIAH Dataset

```bash
cd scripts
python generate_niah_data.py \
  --num_samples 10000 \
  --context_length 512 \
  --needle_type_k words \
  --needle_type_v numbers \
  --output_dir ../data/niah \
  --output_name niah_10k_512.json
```

**Parameters:**
- `--num_samples`: Number of samples to generate
- `--context_length`: Target context length in characters
- `--needle_type_k`: Key type (`words`, `numbers`, `uuids`)
- `--needle_type_v`: Value type (`words`, `numbers`, `uuids`)
- `--depth_distribution`: Needle placement (`uniform`, `random`, `fixed`)

### 2. Generate QA Dataset

```bash
cd scripts
python generate_qa_data.py \
  --num_samples 1000 \
  --context_length 512 \
  --add_distractors \
  --output_dir ../data/qa \
  --output_name qa_1k_512.json
```

**Parameters:**
- `--num_samples`: Number of samples
- `--context_length`: Target context length
- `--add_distractors`: Add distractor paragraphs

### 3. Train NIAH Model

```bash
cd scripts
CUDA_VISIBLE_DEVICES=0 python train_niah.py \
  --data_path ../data/niah/niah_10k_512.json \
  --output_dir ../models/niah_model \
  --epochs 3 \
  --batch_size 1 \
  --learning_rate 2e-5 \
  --grad_accum_steps 4 \
  --gpu_id 0
```

### 4. Train QA Model

```bash
cd scripts
CUDA_VISIBLE_DEVICES=1 python train_qa.py \
  --data_path ../data/qa/qa_1k_512.json \
  --output_dir ../models/qa_model \
  --epochs 3 \
  --batch_size 1 \
  --learning_rate 5e-5 \
  --grad_accum_steps 4 \
  --gpu_id 1
```

## ğŸ“Š RULER Benchmark Metrics

### NIAH Task
- **Metric**: `string_match_all`
- Checks if ALL expected values are present in predictions
- Formula: `(correct_predictions / total) * 100`

### QA Task
- **Metric**: `string_match_part`
- Checks if AT LEAST ONE expected answer is present
- More lenient than `string_match_all`

## ğŸ§¹ Text Cleaning

Following [FlagEmbedding approach](https://github.com/FlagOpen/FlagEmbedding/blob/master/research/Long_LLM/activation_beacon/main/eval_needle.py):

### For Data Generation (RULER-style):
- Remove excessive newlines (`\n\n\n` â†’ `\n\n`)
- Remove excessive spaces
- Remove control characters (`\x00-\x1f`)
- Normalize whitespace

### For Predictions (Flag-style):
- Strip newlines
- Take first line only

## ğŸ“ˆ Training Pipeline

```
1. Data Generation
   â”œâ”€> generate_niah_data.py â†’ data/niah/
   â””â”€> generate_qa_data.py â†’ data/qa/

2. Data Processing
   â””â”€> text_cleaning.py (clean_text_ruler_style)

3. Training
   â”œâ”€> train_niah.py â†’ models/niah_model/
   â””â”€> train_qa.py â†’ models/qa_model/

4. Evaluation
   â””â”€> RULER metrics (string_match_all, string_match_part)

5. Results
   â””â”€> final_results.json (loss, accuracy, examples)
```

## ğŸ”§ Configuration Files

Edit `configs/*.json` to change hyperparameters without modifying code.

**Example** (modify learning rate):
```json
{
  "training": {
    "learning_rate": 1e-4  // Changed from 2e-5
  }
}
```

## ğŸ“ Output Files

After training, each model directory contains:

```
models/niah_model/
â”œâ”€â”€ checkpoint_step_100/
â”‚   â””â”€â”€ model.pt
â”œâ”€â”€ checkpoint_step_200/
â”œâ”€â”€ best_model/
â”‚   â””â”€â”€ model.pt           # Best performing checkpoint
â””â”€â”€ final_results.json      # Evaluation results
```

**final_results.json** structure:
```json
{
  "loss": 3.0,
  "accuracy": 15.5,
  "num_evaluated": 1000,
  "examples": [
    {
      "key": "key_1234",
      "expected_value": "blue car",
      "prediction": "The value is blue car",
      "correct": true
    }
  ]
}
```

## ğŸ¯ Best Practices

### For NIAH:
- Start with small dataset (1K samples) to test
- Use `uniform` depth distribution
- Learning rate: 2e-5 to 1e-4
- Context length: 256-512 chars for initial experiments

### For QA:
- Smaller dataset (1K samples) sufficient
- Higher learning rate (5e-5 to 1e-4)
- Add distractors to make it harder
- Monitor accuracy - should be >30% after training

## âš ï¸ Known Issues

1. **Mixed Precision**: Currently disabled due to GradScaler compatibility issues
2. **Memory**: Precomputing embeddings requires significant RAM
3. **Speed**: ~3-4 it/s during training

## ğŸ“š References

- [RULER Benchmark](https://github.com/hsiehjackson/RULER)
- [FlagEmbedding NIAH](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon)
- [SONAR](https://github.com/facebookresearch/SONAR)

## ğŸ“§ Contact

For questions about this implementation, refer to the training logs and configuration files.

